{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57dcfd55-3f36-4107-8d78-74ef83d9463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import SmoothBivariateSpline\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0877ed-e489-428b-a473-25769e65330d",
   "metadata": {},
   "source": [
    "# Wind Data Upload + Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e623c079-c7a7-4d99-8cba-b8e35e0be448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload wind data\n",
    "wind = pd.read_csv('windproducts.csv', dtype = 'string')\n",
    "## time (UTC, string), altitude (m, string), latitude (degrees north, string), longitude (degrees east, string) \n",
    "### ekman_upwelling (m/s, string), wind_u (m/s, string), wind_v (m/s, string)\n",
    "\n",
    "# Dytpe needs to be float for interpolation, but NaN's need to be removed first\n",
    "df = pd.DataFrame(wind)\n",
    "df = df.drop(0) # first row are units, no actual data\n",
    "df_dropped_subset = df.dropna(subset=['ekman_upwelling', 'wind_u', 'wind_v'])\n",
    "\n",
    "# Create new dataframe with necessary variables (with and without NaNs)\n",
    "## these are used to loop interpolated values into df2\n",
    "wind_df = df_dropped_subset[['latitude', 'longitude', 'ekman_upwelling', 'wind_u', 'wind_v']].astype(float) # creates dataframe without NaNs\n",
    "df2 = df[['latitude', 'longitude', 'ekman_upwelling', 'wind_u', 'wind_v']].astype(float) # creates dataframe with interpolated values\n",
    "\n",
    "# Normalize Data\n",
    "df_mean = wind_df.mean()\n",
    "df_stdev = wind_df.std()\n",
    "\n",
    "wind_df_norm = (wind_df - df_mean) / df_stdev\n",
    "\n",
    "df2_norm = (df2 - df_mean) / df_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c46637e-68a6-43de-bcd8-e8223aea91a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/6s7jmy896l9gc4jq1245rx680000gn/T/ipykernel_1587/2060939219.py:21: UserWarning: ier=988\n",
      "  spline_ekman = SmoothBivariateSpline(lon, lat, ekman, s = 1)\n",
      "/var/folders/00/6s7jmy896l9gc4jq1245rx680000gn/T/ipykernel_1587/2060939219.py:31: UserWarning: ier=1299\n",
      "  spline_u = SmoothBivariateSpline(lon2, lat2, wind_u, s = 0.0005)\n",
      "/var/folders/00/6s7jmy896l9gc4jq1245rx680000gn/T/ipykernel_1587/2060939219.py:41: UserWarning: ier=1299\n",
      "  spline_v = SmoothBivariateSpline(lon, lat, wind_v, s = 0.05)\n"
     ]
    }
   ],
   "source": [
    "# Interpolate wind data using spline interpolation\n",
    "## user warning indicates that interpolation was completed but may have had issues with convergence\n",
    "### ordinary kriging may be prefered but due dataset size, kernel may crash and restart (ensure enough ram is available!)\n",
    "\n",
    "# define variables for simplicity\n",
    "## due to size of data and to avoid overestimation, sample and sample2 were defined \n",
    "### wind_u was overestimated with the sample parameters, however, wind_v became overestimated with sample2 parameters, ekman_upwelling appeared to be fine with either\n",
    "#### using the normalized data also caused overestimation to occur so used non-normalized data to interpret u and v wind\n",
    "\n",
    "sample = wind_df_norm.sample(n = 100, random_state = 50)\n",
    "sample2 = wind_df_norm.sample(n = 1500, random_state = 15)\n",
    "lon = sample['longitude']\n",
    "lat = sample['latitude']\n",
    "ekman = sample['ekman_upwelling']\n",
    "wind_v = sample['wind_v']\n",
    "wind_u = sample2['wind_u']\n",
    "lon2 = sample2['longitude']\n",
    "lat2 = sample2['latitude']\n",
    "\n",
    "# ekman_upwelling interpolation\n",
    "spline_ekman = SmoothBivariateSpline(lon, lat, ekman, s = 1)\n",
    "\n",
    "mask_ekman_nan = df2['ekman_upwelling'].isna()\n",
    "lon_nan = df2_norm.loc[mask_ekman_nan, 'longitude'].values\n",
    "lat_nan = df2_norm.loc[mask_ekman_nan, 'latitude'].values\n",
    "ekman_interp_norm = spline_ekman.ev(lon_nan, lat_nan)\n",
    "ekman_interp = ekman_interp_norm * df_stdev['ekman_upwelling'] + df_mean['ekman_upwelling']\n",
    "df2.loc[mask_ekman_nan, 'ekman_upwelling'] = ekman_interp\n",
    "\n",
    "# wind_u (zonal, wind in the x-direction) interpolation\n",
    "spline_u = SmoothBivariateSpline(lon2, lat2, wind_u, s = 0.0005)\n",
    "\n",
    "mask_u_nan = df2['wind_u'].isna()\n",
    "lon_nan_u = df2_norm.loc[mask_u_nan, 'longitude'].values\n",
    "lat_nan_u = df2_norm.loc[mask_u_nan, 'latitude'].values\n",
    "# u_interp_norm = spline_u.ev(lon_nan_u, lat_nan_u)\n",
    "# u_interp = u_interp_norm * df_stdev['wind_u'] + df_mean['wind_u']\n",
    "df2.loc[mask_u_nan, 'wind_u'] = spline_u.ev(lon_nan_u, lat_nan_u)\n",
    "\n",
    "# wind_v (meridonal, wind in the y-direction) interpolation\n",
    "spline_v = SmoothBivariateSpline(lon, lat, wind_v, s = 0.05)\n",
    "\n",
    "mask_v_nan = df2['wind_v'].isna()\n",
    "lon_nan_v = df2_norm.loc[mask_v_nan, 'longitude'].values\n",
    "lat_nan_v = df2_norm.loc[mask_v_nan, 'latitude'].values\n",
    "# v_interp_norm = spline_v.ev(lon_nan_v, lat_nan_v)\n",
    "# v_interp = v_interp_norm * df_stdev['wind_v'] + df_mean['wind_v']\n",
    "df2.loc[mask_v_nan, 'wind_v'] = spline_v.ev(lon_nan_v, lat_nan_v)\n",
    "# df2.to_csv('check.csv', index = False) # to check interpolated values\n",
    "## may need to restart kernel to get .csv file to show updated results\n",
    "\n",
    "# u and v wind were extremely overestimated, some values were good, but some were 500+ m/s\n",
    "# brining in more samples caused wind_v to overestimate, but wind u was more representative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375952a7-0309-4739-a6f0-7411606e6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this to check values (used to determine why values were getting overestimated)\n",
    "## currently set to check wind v\n",
    "### print(\"Normalized values (min, max):\", v_interp_norm.min(), v_interp_norm.max())\n",
    "### print(\"Unnormalized values (min, max):\", v_interp.min(), v_interp.max())\n",
    "### print(\"Mean:\", df_mean['wind_v'], \"| Std:\", df_stdev['wind_v'])\n",
    "### results showcase there are outliers present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703ceb96-dd34-443d-99ee-518b95fe83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date needs to be added back to dataset, but with time removed\n",
    "## Original time format (string) caused errors\n",
    "df['date'] = pd.to_datetime(df['time']).dt.date\n",
    "date_float = (pd.to_datetime(df['date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1D')\n",
    "date_string = df['date'].astype(str)\n",
    "\n",
    "df2.insert(0, 'date (float)', date_float) # only needs to be done once, will get error if ran twice in the same session\n",
    "df2.insert(1, 'date (string)', date_string) # only needs to be done once, will get error if ran twice in the same session\n",
    "\n",
    "df2[['latitude','longitude', 'wind_u', 'wind_v']] = df2[['latitude','longitude', 'wind_u', 'wind_v']].round(3) # to ensure lon and lat for both datasets are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286fb13-4f02-432b-8047-6c2ff011ced0",
   "metadata": {},
   "source": [
    "# Sea-Surface Temperature Data Upload + Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eaeaa2-cb70-470f-90e2-5224f7e126e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload SST data\n",
    "sst = pd.read_csv('sst.csv', dtype = 'string')\n",
    "# time (UTC, string), zlev (m, string), latitude (degrees north, string), longitude (degrees east, string), sst (degrees C, string)\n",
    "\n",
    "# dytpe needs to be float for interpolation, but NaN's need to be removed first\n",
    "df_ = pd.DataFrame(sst)\n",
    "df_ = df_.drop(0)  # first row is units, not actual data\n",
    "df_dropped_subset2 = df_.dropna(subset = ['sst'])\n",
    "\n",
    "sst_df = df_dropped_subset2[['latitude', 'longitude', 'sst']].astype(float)\n",
    "df3 = df_[['latitude', 'longitude', 'sst']].astype(float)\n",
    "\n",
    "# Normalize the data\n",
    "df_mean2 = sst_df.mean()\n",
    "df_stdev2 = sst_df.std()\n",
    "\n",
    "sst_df_norm = (sst_df - df_mean2) / df_stdev2\n",
    "\n",
    "df3_norm = (df3 - df_mean2) / df_stdev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ba4368-68b5-4046-b9f1-db4fb1ea72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/6s7jmy896l9gc4jq1245rx680000gn/T/ipykernel_1587/221099362.py:11: UserWarning: ier=2116\n",
      "  spline_sst = SmoothBivariateSpline(lon, lat, sst_, s = 1)\n"
     ]
    }
   ],
   "source": [
    "# Interpolate sst data using spline interpolation\n",
    "## user warning indicates that interpolation was completed but may have had issues with convergence\n",
    "\n",
    "# Define variables for simplicity\n",
    "sample = sst_df_norm.sample(n = 1000, random_state = 50)\n",
    "lon = sample['longitude']\n",
    "lat = sample['latitude']\n",
    "sst_ = sample['sst']\n",
    "\n",
    "# SST interpolation\n",
    "spline_sst = SmoothBivariateSpline(lon, lat, sst_, s = 1)\n",
    "\n",
    "mask_sst_nan = df3['sst'].isna()\n",
    "lon_nan = df3_norm.loc[mask_sst_nan, 'longitude'].values\n",
    "lat_nan = df3_norm.loc[mask_sst_nan, 'latitude'].values\n",
    "sst_interp_norm = spline_sst.ev(lon_nan, lat_nan)\n",
    "sst_interp = mask_sst_nan * df_stdev2['sst'] + df_mean2['sst']\n",
    "df3.loc[mask_sst_nan, 'sst'] = sst_interp\n",
    "# df3.to_csv('checkcheck.csv', index = False) # to check interpolated values\n",
    "# no apparent issues with overestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27fb663f-d624-4f5c-ab02-9aea09209d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date needs to be added back to dataset, but with time removed\n",
    "## Original time format caused errors\n",
    "df_['date'] = pd.to_datetime(df_['time']).dt.date\n",
    "date_float = (pd.to_datetime(df_['date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1D')\n",
    "date_string = df_['date'].astype(str)\n",
    "\n",
    "df3.insert(0, 'date (float)', date_float) # only needs to be done once, will get error if ran twice in the same session\n",
    "df3.insert(1, 'date (string)', date_string) # only needs to be done once, will get error if ran twice in the same session\n",
    "\n",
    "df3[['latitude','longitude', 'sst']] = df3[['latitude','longitude', 'sst']].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d02d7-063c-4c40-b683-b68671c8028a",
   "metadata": {},
   "source": [
    "# Merge Wind and SST Datasets + Generate New CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb0783aa-a27a-481d-a306-3d33628716f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the wind and sst datasets \n",
    "## df3 (sst) is being merged to df2 (wind)\n",
    "merged_data = [] # creates a dataframe where the merged datasets will be stored\n",
    "\n",
    "for date in df2['date (string)'].dropna().unique(): # checks unique dates and extracts wind and sst data\n",
    "    wind_day = df2[df2['date (string)'] == date]\n",
    "    sst_day = df3[df3['date (string)'] == date]\n",
    "    merged = pd.merge(wind_day, sst_day[['latitude', 'longitude', 'sst']], on = ['latitude', 'longitude'], how = 'left')\n",
    "    missing_mask = merged['sst'].isna() # checks to see if there are any sst values missing after merge\n",
    "\n",
    "    if missing_mask.any() and not sst_day.empty: # sst and wind have diff lengths, this fills in those NaNs\n",
    "        known_points = sst_day[['latitude', 'longitude']].values\n",
    "        known_values = sst_day['sst'].astype(float).values\n",
    "        missing_points = merged.loc[missing_mask, ['latitude', 'longitude']].values\n",
    "        interpolated = griddata(known_points, known_values, missing_points, method = 'linear')\n",
    "        if np.any(np.isnan(interpolated)):\n",
    "            interpolated_nn = griddata(known_points, known_values, missing_points, method = 'nearest')\n",
    "            interpolated = np.where(np.isnan(interpolated), interpolated_nn, interpolated)\n",
    "        merged.loc[missing_mask, 'sst'] = interpolated\n",
    "        \n",
    "    merged_data.append(merged) # puts everything into the empty merged_data dataframe\n",
    "\n",
    "# Combine all merged days into a single DataFrame\n",
    "merged_data = pd.concat(merged_data, ignore_index=True)\n",
    "merged_data['date (string)'] = pd.to_datetime(merged_data['date (string)'], format='%Y-%m-%d')\n",
    "final_df = merged_data[merged_data['date (string)'] <= '2025-03-29']\n",
    "#final_df.to_csv('wind_sst.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed2e26-ff8d-410c-be33-0bf501b4a5b0",
   "metadata": {},
   "source": [
    "# Defining Upwelling Parameters + Generating New CSV File\n",
    "##### For Upwelling to occur in the California Current System\n",
    "##### - Duration: >12 hours or more of specific wind direction and speed (for this dataset, 1+ days and 3+ day)*\n",
    "##### - Magnitude: >5 m/s \n",
    "##### - Direction: Northwesterly / Equatorward (wind_v < 0, wind_u > 0)**\n",
    "######\n",
    "###### *upwelling duration is typically 1-2 weeks, chose 3+ days to determine upwelling event to avoid overestimation\n",
    "###### **wind_v and wind_u are the meridonal and zonal winds, primarily indicates direction so calculated wind_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e47233-e018-4ac5-9ed1-b6114794ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data \n",
    "upwelling = pd.read_csv('wind_sst.csv')\n",
    "upwelling_df = pd.DataFrame(upwelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae04ad70-972d-4871-a428-5784cdac06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "wind_u = upwelling_df['wind_u']\n",
    "wind_v = upwelling_df['wind_v']\n",
    "sst = upwelling_df['sst']\n",
    "date = pd.to_datetime(upwelling_df['date (string)'])\n",
    "\n",
    "# Create wind speed variable using u and v wind\n",
    "upwelling_df['wind_speed'] = np.sqrt(upwelling_df['wind_u']**2 + upwelling_df['wind_v']**2) # need for wind magnitude\n",
    "wind_speed = upwelling_df['wind_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee651fe-27ff-4167-baef-900298e559bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sst decrease\n",
    "## code from non-spatial section was used to elaborate for spatial data\n",
    "### code groups all data by unique lat and lon pair, and sorts via the date, then makes notes if SST dropped the day before or not (T or F)\n",
    "#### sst drops in response to upwelling bring colder bottom waters to the surface, acts more of a check than a parameter for upwelling\n",
    "upwelling_df = upwelling_df.sort_values(['latitude', 'longitude', 'date (string)']).copy()\n",
    "\n",
    "# Compute SST change at each location over time\n",
    "upwelling_df['sst_diff'] = (upwelling_df.groupby(['latitude', 'longitude'])['sst'].diff())\n",
    "upwelling_df['sst_decrease'] = upwelling_df['sst_diff'] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b2b7c6-cf7d-4c79-9fb3-99922ab2e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining upwelling based on wind speed, u wind, v wind and sst based on 1 day\n",
    "## CCS upwelling periods are 1-2 weeks, but upwelling events can also occur for longer periods\n",
    "def upwelling(wind_u, wind_v, wind_speed, sst_decrease):    \n",
    "    if (wind_speed > 5 and wind_u > 0 and wind_v < 0 and sst_decrease == True):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Create new column upwelling1day in upwelling_df\n",
    "## lambda row essentially loops through each row to see if parameters as defined in upwelling is met\n",
    "upwelling_df['upwelling1day'] = upwelling_df.apply(lambda row: upwelling(row['wind_u'], row['wind_v'], row['wind_speed'], row['sst_decrease']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae2e1ee-0481-4d81-8163-ae8f4e89765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/6s7jmy896l9gc4jq1245rx680000gn/T/ipykernel_1587/3321504030.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  upwelling_df = upwelling_df.groupby(['latitude', 'longitude'], group_keys = False).apply(upwelling3day)\n"
     ]
    }
   ],
   "source": [
    "# Defining upwelling based on wind speed, u wind, v wind and sst based on 3 days or more of parameters being met\n",
    "upwelling_df = upwelling_df.sort_values(['latitude', 'longitude', 'date (string)']).copy()\n",
    "\n",
    "def upwelling3day(group):\n",
    "    group_id = (group['upwelling1day'] != group['upwelling1day'].shift()).cumsum()\n",
    "    group_sizes = group.groupby(group_id)['upwelling1day'].transform('sum')\n",
    "    group['upwelling3day'] = np.where((group['upwelling1day'] == 1) & (group_sizes >= 3),1,0 )\n",
    "    return group\n",
    "\n",
    "# Function is applied to each coordinate group\n",
    "upwelling_df = upwelling_df.groupby(['latitude', 'longitude'], group_keys = False).apply(upwelling3day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fc245-8134-4fab-b964-ef2a079d3816",
   "metadata": {},
   "source": [
    "## Defining Upwelling Non-Spatially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "444a6ded-2ae6-4ab1-975f-62f2bcfb8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new csv file\n",
    "upwelling_df1 = upwelling_df.round({'wind_u':4, 'wind_v':3, 'wind_speed':3, 'sst':2,'wind_speed':3})\n",
    "#upwelling_df1.to_csv('upwelling_spatial.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f5a19b-2c05-4e06-b020-f1148cb4d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine which lon and lat pairs appear in each day of the dataset\n",
    "#total_dates = upwelling_df['date (float)'].nunique()\n",
    "#pair_counts = (upwelling_df.groupby(['latitude', 'longitude'])['date (float)'].nunique().reset_index(name='date_count'))\n",
    "#consistent_pairs = pair_counts[pair_counts['date_count'] == total_dates][['latitude', 'longitude']]\n",
    "#consistent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2a621ed-cd47-4041-bad4-6d95c48dcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new non-spatial dataset\n",
    "upwelling1 = pd.read_csv('wind_sst.csv')\n",
    "upwelling_df1 = pd.DataFrame(upwelling1)\n",
    "\n",
    "lat = 34.833\n",
    "lon = -123.833\n",
    "df_filtered = upwelling_df1[(upwelling_df1['latitude'] == lat) & (upwelling_df1['longitude'] == lon)].copy()\n",
    "df_filtered = df_filtered.dropna()\n",
    "df_filtered = df_filtered.iloc[1:].reset_index(drop=True) # dropped first row as it goes from 2021-09-03 to 2021-09-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652318c1-babd-4ab2-a749-3cc1ec76c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "wind_u = df_filtered['wind_u']\n",
    "wind_v = df_filtered['wind_v']\n",
    "sst = df_filtered['sst']\n",
    "\n",
    "# Create wind speed variable from u and v wind\n",
    "df_filtered['wind_speed'] = np.sqrt(df_filtered['wind_u']**2 + df_filtered['wind_v']**2) # need for wind magnitude\n",
    "wind_speed = df_filtered['wind_speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d00f191-1972-4b81-b897-03bc75e243e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column that determines if sst drops (T or F)\n",
    "df_filtered['sst_diff'] = df_filtered['sst'].diff()\n",
    "df_filtered['sst_decrease'] = df_filtered['sst_diff'] < 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bddb5fb-3343-451e-86ab-8fe4cb95f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining upwelling based on wind speed, u wind, v wind and sst based on 1 day\n",
    "def upwelling3(wind_u, wind_v, wind_speed, sst_decrease):    \n",
    "    if (wind_speed > 5 and wind_u > 0 and wind_v < 0 and sst_decrease == True):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_filtered['upwelling1day'] = df_filtered.apply(lambda row: upwelling3(row['wind_u'], row['wind_v'], row['wind_speed'], row['sst_decrease']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7392ded1-a2cb-49b4-9bc8-9a8a26c24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining upwelling based on wind speed, u wind, v wind and sst based on 3 days or more of parameters being met\n",
    "df_filtered = df_filtered.sort_values('date (string)').reset_index(drop = True) # sorts by day\n",
    "group_id = (df_filtered['upwelling1day'] != df_filtered['upwelling1day'].shift()).cumsum() # creates an id variable to find consecutive days \n",
    "group_sizes = df_filtered.groupby(group_id)['upwelling1day'].transform('sum') # determines the length of consecutive days\n",
    "df_filtered['upwelling3day'] = np.where((df_filtered['upwelling1day'] == 1) & (group_sizes >= 3),1,0) # creates new column that only includes when parameters are met for 3+ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70c7e648-8c2a-4d9d-83a7-e7a76fcbfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new csv file\n",
    "#df_filtered.to_csv('upwelling_nonspatial.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
